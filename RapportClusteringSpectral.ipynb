{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les algorithmes de clustering sont des outils d'analyse des données qui permettent de grouper des échantillons, de sorte que les membres d'un même groupe (ou *cluster*) sont très similaires, alors que ceux de groupes différents ne le sont pas. Ces algorithmes ont des applications diverses et variées, en biologie (étude des espèces et analyse génétique), en médecine (reconnaissance d'image), en commerce (étude de marché et conception de produit) et en bien d'autres domaines.\n",
    "\n",
    "Dans la suite, nous allons préciser explorer deux algorithmes de *clustering* : Kmeans et Clustering Spectral. Les deux seront implémentés et comparés sur des données différentes afin d'élucider leurs différences, ainsi que leurs points faibles et leurs points forts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kmeans est réputé comme l'algorithme de clustering le plus populaire. Il doit sa popularité à sa simplicité et à son efficacité, même si la précision du résultat n'est pas garantie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Étant donné un entier k et un ensemble $X$ de n points de $\\mathbb{R}^d$, le but est de choisir k centres $C = \\{c_1,c_2...c_k\\}$ qui minimisent la fonction : $$\\phi(C) = \\sum_{x\\in X} \\min_{c\\in C} \\|x_i,c\\|^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme classique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme Kmeans donne une solution simple et rapide de ce problème :\n",
    "\n",
    "1. Choisir uniformément aribitrairement k centres $C = {c_1,c_2...,c_k}$ parmi $X$\n",
    "2. Pour tout $i\\in\\{1..k\\}$, construire $C_i$ l'ensemble des points de $X$ qui sont plus proches de $c_i$ que de n'importe quel autre centre\n",
    "3. Mettre à jour $c_i := \\dfrac{1}{|C_i|}\\sum_{x\\in C_i} x$\n",
    "4. Répéter les étapes 2 et 3 jusqu'à convergence (i.e. $C$ ne change plus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idée de l'algorithm repose sur le fait que les étapes 2 et 3 garantissent que $\\phi$ diminue à chaque itération. Le choix initial de $C$ est par contre crucial et peut potentiellement aboutir sur un minimum local de $\\phi$ au lieu d'approximer un minimum global.\n",
    "\n",
    "On peut déjà voir que l'algorithme est adapté aux structures de données où les clusters se présentent comme des regroupements compacts et risque d'avoir des difficultés à distinguer des clusters qui sont imbriqués les uns dans les autres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Algorithme amélioré : Kmeans ++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme Kmeans++ propose un choix plus intelligent des centres initiaux :\n",
    "\n",
    "1.  a. Choisir le centre $c_1$ uniformément parmi $X$\n",
    "    \n",
    "    b. Ayant choisi $i$ centres $C_i = \\{c_1,c_2...,c_i\\}$, choisir le centre suivant parmi $x\\in X$ avec probabilité $$\\frac{\\min_{c\\in C_i} \\|x,c\\|^2}{\\sum_{y\\in X} \\min_{c\\in C_i} \\|y,c\\|^2}$$\n",
    "    \n",
    "    c. Répéter l'étape b jusqu'à obtenir $k$ centres $C_k$\n",
    "\n",
    "2-4. Identiques à l'algorithme classique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme suivant, nommé Kmeans++**[1]**, comporte une amélioration de la première étape qui permet d'obtenir des résultats plus consistants, tout en augmentant la vitesse de convergence en moyenne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $D^2$ random distribution :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following algorithm returns a random index using a discrete (non negative) weight distribution w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random = function(w){\n",
    "    n = length(w)\n",
    "    # discrete cumulative distribution \n",
    "    p = cumsum(w)\n",
    "    # simulate probability using Inverse Transform Sampling\n",
    "    u = runif(1,0,p[n])\n",
    "    # find the index of the inverse using dichotomic search\n",
    "    a = 0\n",
    "    b = n\n",
    "    # loop invariant : a<b and p[a]<=u<p[b]\n",
    "    while(b-a>1){\n",
    "        c = a + (b-a)%/%2\n",
    "        if (p[c]>u){\n",
    "            b = c\n",
    "        } else {\n",
    "            a = c\n",
    "        }\n",
    "    }\n",
    "    return(b)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidien distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function computes the square of the euclidien distance between two vectors x and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist2 = function(x, y){\n",
    "    return (t(x-y)%*%(x-y))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Cluster partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The following algorithm returns the indexing vector for the clusters obtained by partitioning the dataset *X* using a set of cluster centers *C*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iclusters = function(X, C){\n",
    "    # sample size\n",
    "    n = ncol(X)\n",
    "    # cluster count\n",
    "    k = ncol(C)\n",
    "    # Cindex will contain the index of the nearest center\n",
    "    Cindex = vector(\"double\", n)\n",
    "    # partitioning sample over cluster centers\n",
    "    for (x in 1:n){\n",
    "        d = dist2(X[,x],C[,1])\n",
    "        Cindex[x] = 1\n",
    "        for (c in 2:k){\n",
    "            if (dist2(X[,x],C[,c]) < d){\n",
    "                d = dist2(X[,x],C[,c])\n",
    "                Cindex[x] = c\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return(Cindex)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The following algorithm returns the list of clusters obtained by partitioning the dataset *X* using the indexing vector *index*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters = function(X, index){\n",
    "    # cluster count\n",
    "    k = ncol(C)\n",
    "    # constructing the list of clusters\n",
    "    clist = vector(\"list\", k)\n",
    "    for (c in 1:k){\n",
    "        cindex = Cindex==c\n",
    "        clist[[c]] = X[,cindex]\n",
    "    }\n",
    "    return(clist)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $D^2$ seeding method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following algorithm returns a matrix of k randomly chosen initial centers from a dataset X, using the D² seeding method which works recursively as follows :\n",
    "\n",
    "* Choose the first center $C_1$ uniformely at random from X\n",
    "\n",
    "* Given the first $C = \\{C_1,...C_{i-1}\\}$ centers chosen, choose the next center $C_i$ at random from X using the probability distribution :\n",
    "$\\bigg(\\dfrac{D(x)^2}{\\sum_{x\\in X} D(x)^2}\\bigg)_{x\\in X}$ where $D(x) = \\min\\big\\{||x-c||:c\\in C\\big\\}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d2seed = function(X, k){\n",
    "    # sample dimensions\n",
    "    d = nrow(X)\n",
    "    n = ncol(X)\n",
    "    # allocate a dxk matrix for the centers\n",
    "    C = matrix(NA_real_, d, k)\n",
    "    # choose first center uniformely from the sample\n",
    "    i = floor(runif(1,0,n))\n",
    "    C[,1] = X[,i+1]\n",
    "    # calculate initial weight distribution\n",
    "    d2 = vector(\"double\", n)\n",
    "    for (i in 1:n){\n",
    "        d2[i] = dist2(X[,i],C[,1])\n",
    "    }\n",
    "    # choose the next (k-1) remaining centers using the D² method\n",
    "    for (c in 2:k){\n",
    "        # select the next center using auxiliary function\n",
    "        r = random(d2)\n",
    "        C[,c] = X[,r]\n",
    "        # update weight distribution\n",
    "        for (i in 1:n){\n",
    "            d2[i] = min(d2[i], dist2(X[,i],C[,c]))\n",
    "        }\n",
    "    }\n",
    "    return(C)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans++ algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following algorithm returns a matrix of k cluster centers for a dataset X, using the Kmeans algorithm initialized with the $D^2$ seeding method. The iter argument specifies the maximum number of iterations of the optimization loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmpp = function(X, k, iter=0){\n",
    "    # sample size\n",
    "    n = ncol(X)\n",
    "    # initialization of cluster centers\n",
    "    C = d2seed(X,k)\n",
    "    # optimization loop\n",
    "    repeat{\n",
    "        # Cindex will contain the index of the nearest scenter\n",
    "        Cindex = vector(\"double\",n)\n",
    "        # partitioning sample over cluster centers\n",
    "        for (x in 1:n){\n",
    "            d = dist2(X[,x],C[,1])\n",
    "            Cindex[x] = 1\n",
    "            for (c in 2:k){\n",
    "                if (dist2(X[,x],C[,c]) < d){\n",
    "                    d = dist2(X[,x],C[,c])\n",
    "                    Cindex[x] = c\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        # updating cluster centers\n",
    "        convergence = TRUE\n",
    "        for (c in 1:k){\n",
    "            index = Cindex==c\n",
    "            size = sum(index)\n",
    "            cluster = X[,index]\n",
    "            if (size>1){\n",
    "                newcenter = rowSums(cluster)/size\n",
    "            } else {\n",
    "                newcenter = cluster\n",
    "            }            \n",
    "            if (all(newcenter!=C[,c])){\n",
    "                convergence = FALSE\n",
    "                C[,c] = newcenter\n",
    "            }\n",
    "        }\n",
    "        iter = iter-1\n",
    "        if(convergence || iter==0){\n",
    "            break\n",
    "        }\n",
    "    }\n",
    "    return(C)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Spectral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définitions basiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation graphe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soit $G = (V,E)$ un graphe non orienté avec $V = \\{v_1, . . . ,v_2\\}$ l'ensemble des sommets.\n",
    "Dans la partie suivante, on suppose que le graphe $G$ est muni de poids, que chaque arête entre deux sommets $v_i$ et $v_j$ soit de poid non negative $w_{ij} ≥ 0$.\n",
    "La matrice adjacente du graphe $G$ est la matrice $W = (w_{ij})$ $i,j=1,...,n$.\n",
    "\n",
    "Le degrée d'un sommet $v_i \\in V$ est defini par $d_i = \\sum\\limits_{j=1}^n w_{ij}$.\n",
    "\n",
    "Le degrée matricielle $D$ est défini par une matrice diagonale ayant pour diagonales les degrées $d_1, . . . , d_n$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphe de similitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a plusieurs façon de transformer un ensemble donné de points $x_1, . . . , x_n$ doté d'une matrice de similarités $s_{ij}$ ou d'une matrice de distance $d_{ij}$ en un graphe. Lorsque l'on construit un graphe de similitude, le but est de modéliser la relation de voisin proche entre les points.\n",
    "\n",
    "- Le ε-voisinage graphe : On connecte tous les points dont la distance l'un l'autre est plus petite que ε.\n",
    "\n",
    "- Le k-plus proche voisin graphe : Ici le but est de connecter $v_i$ avec le sommet $v_j$ if $v_j$ est parmi les k-plus proches voisins de $v_i$ mais comme la relation de voisin n'est pas forcément symetrique, on considère deux différentes constructions.\n",
    "\n",
    "    - Le k-plus proche voisin graphe : On ignore les directions des arêtes, on connecte $v_i$ et $v_j$ avec une arête non orientée si $v_i$ est parmi les k-plus proches voisins de $v_j$ ou si $v_j$ est parmi les k-plus proches voisins de $v_i$.\n",
    "    - Le mutuel k-plus proche voisin graphe : On connecte les sommets $v_i$ et $v_j$ si on a $v_i$ est parmi les k-plus proches voisins de $v_j$ et si $v_j$ est parmi les k-plus proches voisins de $v_i$.\n",
    "\n",
    "-Le graphe complet : Ici on connecte simplement l'ensemble des points avec une similaritée positive, le poids de chaque arête étant $s_{ij}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphe Laplaciens et leurs propriétés basiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la partie suivante on suppose toujours que $G$ est non orienté, pondéré de la matrice $W$ avec $w_ij = w_ji ≥ 0$.\n",
    "\n",
    "La matrice du graphe Laplacien non normalisé est défini par $L = D − W$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions auxiliaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphes de similitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La fonction suivante retourne un ε-voisinage graphe, étant données une matrice de distance *d* et un seuil *e*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "egraph = function(d,e){\n",
    "    n=nrow(d)\n",
    "    for(i in 1:n){\n",
    "        for(j in i:n){\n",
    "            if(d[i,j]>e){\n",
    "                d[i,j]=0\n",
    "                d[j,i]=0\n",
    "            } else {\n",
    "                d[i,j]=1\n",
    "                d[j,i]=1\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return(d)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La fonction suivante retourne un k-plus proche voisin graphe, étant données une matrice de distance *d* et un nombre de voisins *k*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in parse(text = x, srcfile = src): <text>:25:81: '{' inattendu(e)\n24:                     if(mutual){\n25:                         if((d[i,j]<min && M[i,j]==0 && d[j,i]<min2 && M[j,i]==0){\n                                                                                    ^\n",
     "output_type": "error",
     "traceback": [
      "Error in parse(text = x, srcfile = src): <text>:25:81: '{' inattendu(e)\n24:                     if(mutual){\n25:                         if((d[i,j]<min && M[i,j]==0 && d[j,i]<min2 && M[j,i]==0){\n                                                                                    ^\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "knearest = function(k,d,mutual=FALSE){\n",
    "    n=nrow(d)\n",
    "    #Matrix n*p of zero\n",
    "    M=matrix(data=numeric(n*n),ncol=n,nrow=n)\n",
    "    for(i in 1:n){\n",
    "        #Iterate the number of point that we have to connect\n",
    "        kk=k\n",
    "        while(kk>0){\n",
    "            #Cordinates of the nearest point of i\n",
    "            #Start at (i,1)\n",
    "            ii=i\n",
    "            if(i==1){ jj=2 }\n",
    "            else{ jj=1 }\n",
    "            if(mutual){\n",
    "                min2=d[jj,ii] \n",
    "                min=d[ii,jj] \n",
    "            }\n",
    "            else{\n",
    "                min=d[ii,jj]\n",
    "            }\n",
    "            #Found the cordinates of the nearest point of i\n",
    "            for(j in 1:n){\n",
    "                if(j!=i){\n",
    "                    if(mutual){\n",
    "                        if((d[i,j]<min && M[i,j]==0 && d[j,i]<min2 && M[j,i]==0){\n",
    "                            min=d[i,j]\n",
    "                            min2=d[j,i]\n",
    "                            ii=i\n",
    "                            jj=j\n",
    "                        }\n",
    "                    }\n",
    "                    else{\n",
    "                        if((d[i,j]<min && M[i,j]==0)){\n",
    "                            min=min(d[i,j],d[j,i])\n",
    "                            ii=i\n",
    "                            jj=j\n",
    "                        }\n",
    "\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            kk=kk-1\n",
    "            #M take the nearest point which is at (ii,jj)\n",
    "            if(mutual){\n",
    "                \n",
    "            }\n",
    "            else{\n",
    "                M[ii,jj]=d[ii,jj]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return(M)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice des degrées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La fonction suivante calcule la matrice des degrées pour une matrice de similitude *W*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "degree = function(W){\n",
    "    n=nrow(W)\n",
    "    #Allocate a zero matrix length n*n\n",
    "    D = matrix(data=numeric(n*n), nrow=n, ncol=n)\n",
    "    for(i in 1:n){\n",
    "        D[i,i]=sum(W[i,])\n",
    "    }\n",
    "    return(D)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice Laplacien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction suivante calcul la matrice Laplacien d'une matrice de similitude *W*. L'argument *normalize* spécifie la méthode de normalisation utilisée pour la matrice Laplacien. Par défaut cela est mis à 0 (non normalisé)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "laplacian = function(W,normalize=0){\n",
    "    #length of the matrix W\n",
    "    n=ncol(W)\n",
    "    #degree matrix\n",
    "    D=degree(W)\n",
    "    L=D-W\n",
    "    #Normalized Laplacian Lsym\n",
    "    if(normalize==1){\n",
    "        RD=solve(D^(1/2))\n",
    "        return(RD*L*RD)\n",
    "    }\n",
    "    #Normalized Laplacian Lrw\n",
    "    if(normalize==2){\n",
    "        return(solve(D)*L)\n",
    "    }\n",
    "    #Unormalized Laplacian of W\n",
    "    return(L)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k premiers vecteurs propres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction suivante retourne les *k* premiers vecteurs propres (dans l'ordre de leur valeur propres correspondantes) pour donnée une matrice *L*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eigenvectors = function(L,k){\n",
    "    #Compute all the eigenvalues and eigenvectors of L\n",
    "    X=eigen(L)\n",
    "    #Take the first k eigenvectors\n",
    "    t=length(X$values)\n",
    "    return(X$vectors[,(t-k+1):t])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme de Clustering Spectral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idée cachée derière l'algorithme de Clustering Spectral est de translaté les données dans l'espace vectoriel $\\mathbb{R}^k$ en utilisant les graphes Laplaciens. Ce changement de perspective, qui dans certains cas est du au propriétés des graphes Laplaciens, améliore les resultats du clustering obtenue à travers le clustering de k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction suivante implémente les algorithmes de Clustering Spectral. Elle retourne un *k*-cluster indexant les vecteurs pour une matrice de similitude *S*. L'argument *normalize* peut-être utilisé pour spécifier une méthode de normalisation de Laplacien (1 for $L_{rw}$ and 2 for $L_{sym}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spec = function(S,k,normalize=0){\n",
    "    # Weighted adjacency matrix of S\n",
    "    W=S\n",
    "    # Compute the normalized Laplacian Lrw\n",
    "    L=laplacian(W,normalize)\n",
    "    # Compute the first k-eigenvectors of Lrw\n",
    "    U=eigenvectors(L,k)\n",
    "    # Normalize the rows\n",
    "    if(normalize==2){\n",
    "        n = nrow(U)\n",
    "        N=(rowSums(U^2)^(1/2))\n",
    "        for(i in 1:n){\n",
    "            if (N[i]>0){\n",
    "                for(j in 1:k){\n",
    "                    U[i,j]=U[i,j]/N[i]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    # Transpose the matrix U\n",
    "    Y=t(U)\n",
    "    # Cluster the points with k-means algorithm\n",
    "    C=kmpp(Y,k)\n",
    "    return(iclusters(Y,C))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Application Numérique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function performs a multidimentional plot of *k* clusters obtained from a dataset *X* using the indexing vector *index*.\n",
    "* The *names* vector contains the labels for data coordinates\n",
    "* The *title* string specifies the title of the image\n",
    "* The *col* vector specifies cluster colors\n",
    "* The *shape* value specifies the shape of the dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pclusters = function(X, k, index, names, title, col = NULL, shape=1){\n",
    "    # dataset dimension\n",
    "    d = nrow(X)\n",
    "    # computing boundaries for the data\n",
    "    blist = vector(\"list\",d)\n",
    "    for(i in 1:d){\n",
    "        blist[[i]] = c(min(X[i,]),max(X[i,]))\n",
    "    }\n",
    "    # constructing the list of clusters\n",
    "    clist = vector(\"list\", k)\n",
    "    for (c in 1:k){\n",
    "        bool = index==c\n",
    "        clist[[c]] = X[,bool]\n",
    "    }\n",
    "    # plotting colors\n",
    "    if (length(col)==0){\n",
    "        col = c(2:(k+1))\n",
    "    }\n",
    "    # adjusting margins\n",
    "    par(oma=c(2,2,5,2), mar=(c(0,0,0,0)+.5))\n",
    "    # plotting in R²\n",
    "    if (d==2){\n",
    "        plot(clist[[1]][1,],clist[[1]][2,], col = col[1],\n",
    "             pch=shape, xlab=names[1], ylab=names[2], \n",
    "             xlim = blist[[1]], ylim = blist[[2]])\n",
    "        for (c in 2:k){\n",
    "            points(clist[[c]][1,],clist[[c]][2,], col = col[c],\n",
    "                   xlab=names[1], ylab=names[2], pch=shape)\n",
    "        }\n",
    "        title(title, outer=TRUE)\n",
    "    }\n",
    "    # plotting in higher dimensions\n",
    "    else {\n",
    "        par(mfrow=c(d,d))\n",
    "        for (i in 1:d){\n",
    "            for (j in 1:d){\n",
    "                # plotting dimension name\n",
    "                if (j==i){\n",
    "                    plot(blist[[i]], blist[[i]], #ann = F,  \n",
    "                         type = 'n', xaxt = 'n', yaxt = 'n',)\n",
    "                    text(x = mean(blist[[i]]), y = mean(blist[[i]]),\n",
    "                         paste(names[i]), cex = 1.25, col = \"black\")\n",
    "                } else {\n",
    "                    # plotting the clusters in [j,i] space\n",
    "                    if(length(nrow(clist[[1]]))>0){\n",
    "                        plot(clist[[1]][j,],clist[[1]][i,], ann=F,\n",
    "                             col = col[1], xaxt='n', yaxt='n', \n",
    "                             pch=shape, xlim = blist[[j]], \n",
    "                             ylim = blist[[i]])\n",
    "                        for (c in 2:k){\n",
    "                            if(length(nrow(clist[[c]]))>0){\n",
    "                                points(clist[[c]][j,],clist[[c]][i,], \n",
    "                                       ann=F, pch=shape, col = col[c],\n",
    "                                       xaxt='n', yaxt='n')\n",
    "                            }\n",
    "                        }\n",
    "                    } else {\n",
    "                        plot(clist[[2]][j,],clist[[2]][i,], ann=F,\n",
    "                         col = col[1], xaxt='n', yaxt='n', pch=shape, \n",
    "                         xlim = blist[[j]], ylim = blist[[i]])\n",
    "                        for (c in 3:k){\n",
    "                            if(length(nrow(clist[[c]]))>0){\n",
    "                                points(clist[[c]][j,],clist[[c]][i,],\n",
    "                                       ann=F, pch=shape, col = col[c],\n",
    "                                       xaxt='n', yaxt='n')\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    # drawing axes\n",
    "                    if (i==d){\n",
    "                        axis(1)\n",
    "                    } else if (i==1){\n",
    "                        axis(3)\n",
    "                    }\n",
    "                    if (j==d){\n",
    "                        axis(4)\n",
    "                    } else if (j==1){\n",
    "                        axis(2)\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        title(title, outer=TRUE)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a two-dimentional simulated clustering dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# install.packages(\"mlbench\")\n",
    "library(mlbench)\n",
    "set.seed(111)\n",
    "obj = mlbench.spirals(100,1,0.025)\n",
    "X =  t(4 * obj$x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Clustering result using Kmeans++ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clustering\n",
    "kmcluster = iclusters(X,kmpp(X,2))\n",
    "# plotting\n",
    "pclusters(X,2,kmcluster,c(\"X1\",\"X2\"), col=c(2,4),\n",
    "          \"Kmeans Clustering : Simulated dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Clustering using Spectral Clustering :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# distance matrix\n",
    "D = matrix(0,n,n)\n",
    "for (i in 1:n){\n",
    "    for (j in 1:n){\n",
    "        D[i,j] = exp(-8*dist2(X[,i],X[,j]))\n",
    "    }\n",
    "}\n",
    "# ε-neighborhood graph\n",
    "e = .5\n",
    "S = egraph(D,e)\n",
    "# clustering\n",
    "spcluster = spec(D,2,0)\n",
    "# plotting\n",
    "pclusters(X,2,spcluster,c(\"X1\",\"X2\"), col=c(2,4),\n",
    "          \"Unnormalized Spectral Clustering : Simulated dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spectral clustering algorithm clearly outperforms kmeans++ in this simulated example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris Flower dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second numerical application, we use Fisher's Iris Flower Dataset. It consists of three classes of Iris flowers : \n",
    "    - Iris Setosa\n",
    "    - Iris Versicolour\n",
    "    - Iris Virginica\n",
    "\n",
    "Each class contains 50 instances with the following measurements :\n",
    "    1. sepal length in cm\n",
    "    2. sepal width in cm\n",
    "    3. petal length in cm\n",
    "    4. petal width in cm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = read.csv(file=\"iris.data\",head=F,sep=\",\")\n",
    "d$V5<-factor(d$V5)\n",
    "summary(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 150\n",
    "X = t(matrix(c(d$V1,d$V2,d$V3,d$V4),nrow=n,ncol=4))\n",
    "index = as.numeric(d$V5)\n",
    "t = \"Iris flower data (red=setosa,green=versicolor,blue=virginica)\"\n",
    "pclusters(X,3,index,c(\"Sepal length\",\"Sepal width\",\n",
    "                      \"Petal length\",\"Petal width\"),\n",
    "          t ,shape=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Clustering using Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clustering\n",
    "kmcluster = iclusters(X,kmpp(X,3))\n",
    "# plotting\n",
    "pclusters(X,3,kmcluster,c(\"Sepal length\",\"Sepal width\",\n",
    "                          \"Petal length\",\"Petal width\"),\n",
    "          \"Kmeans Clustering : Iris Flower data\", shape=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Clustering using Spectral Clustering :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# distance matrix\n",
    "D = matrix(0,n,n)\n",
    "for (i in 1:n){\n",
    "    for (j in 1:n){\n",
    "        D[i,j] = dist2(X[,i],X[,j])\n",
    "    }\n",
    "}\n",
    "# ε-neighborhood graph\n",
    "e = 1\n",
    "S = egraph(D,e)\n",
    "# clustering\n",
    "spcluster = spec(S,3,0)\n",
    "# plotting\n",
    "pclusters(X,3,spcluster,c(\"Sepal length\",\"Sepal width\",\n",
    "                          \"Petal length\",\"Petal width\"), \n",
    "          \"Unnormalized Spectral Clustering : Iris Flower data\",\n",
    "          shape=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Preliminary conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So far, we can see that Kmeans++ outperforms Spectral Clustering on the Iris Flower dataset. Further study of the behavior of spectral clustering is needed to better understand its sensitivity to certain parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[1]** Arthur D, Vassilvitskii S (2007) k-means++: the advantages of careful seeding. Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms. pp 1027--1035.\n",
    "\n",
    "**[2]** von Luxburg, U., A Tutorial on Spectral Clustering, in *Statistics and Computing*, 17 (4), 2007.\n",
    "\n",
    "\n",
    "**[3]** Fisher,R.A. \"The use of multiple measurements in taxonomic problems\" Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to Mathematical Statistics\" (John Wiley, NY, 1950).\n",
    "\n",
    "**[4]** L. Zelnik-Manor and P.  Perona. Self-tuning spectral clustering. In L. K. Saul, Y.  Weiss, and L. Bottou, editors, Advances in Neural Information Processing  Systems 17 , pages 1601–1608. MIT  Press, Cambridge, MA,2005."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
